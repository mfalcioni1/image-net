{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('torchvision')",
   "metadata": {
    "interpreter": {
     "hash": "5c23c9b6a5fcb081ad07781efffc21cb9b69e245b6e4111ab23ab95c76e3bceb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in C:\\Users\\Matt/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "source": [
    "We do some simple logic flow to handle either png or jpg images. The pre-trained torch models require images to be normalized into mini-batches of 3-channel RGB images of shape `(3xHxW)` where `H` and `W` are atleast `224`. The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'RGB'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "url, filename = (\"https://user-images.githubusercontent.com/15249120/107858311-9b08cd00-6e01-11eb-9891-6caaac901de7.png\", \"./img/hp.png\")\n",
    "#url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "input_image = Image.open(filename)\n",
    "if input_image.mode != 'RGB':\n",
    "    input_image = input_image.convert('RGB')\n",
    "input_image.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e+00,  1.2605e+00,\n         1.6055e+00,  4.0594e+00, -3.7257e+00,  1.9499e-01,  5.7948e+00,\n        -4.5434e+00,  1.3774e+00,  1.3026e+00,  5.4394e-01, -1.8425e+00,\n        -1.0543e+00,  2.0610e+00,  1.7900e+00,  3.3995e+00,  5.6849e+00,\n         1.9958e+00,  4.9095e-01, -4.9116e-01,  1.9823e+00, -2.4414e+00,\n        -1.4520e+00,  3.9616e+00, -4.4381e-01, -3.8214e-01, -3.3330e-01,\n         5.3863e+00,  2.9159e-01, -5.9827e-01,  2.7048e+00, -9.1224e-01,\n         3.0214e-01,  2.9702e+00,  4.3191e+00,  1.2361e+00,  1.5278e-01,\n        -2.0109e+00, -1.2003e+00, -2.9844e+00,  3.4420e-01,  4.2787e+00,\n         2.8461e+00,  2.6912e-01,  2.2900e+00,  2.0522e-01,  1.8382e+00,\n         1.0867e+00,  3.7735e+00,  1.0517e+00,  3.4661e+00,  1.5830e+00,\n        -3.7087e+00,  2.2527e+00,  8.2049e-02,  2.0804e-01, -3.5766e-01,\n         3.3546e+00,  7.9393e-01,  6.3751e-01,  1.1036e+00, -1.7985e+00,\n         2.1818e+00,  3.7322e+00, -3.4788e+00,  3.0517e+00, -1.4758e+00,\n         5.6887e+00,  4.6413e+00,  2.8413e+00, -4.4065e-01,  5.7858e+00,\n         3.4644e+00,  2.9751e-01,  1.5529e+00,  2.0805e+00,  1.3711e-02,\n         2.9233e+00,  2.6804e+00,  2.0069e+00,  2.6240e+00,  9.8768e-01,\n         8.6642e-01,  2.1918e+00, -2.5307e+00, -1.2556e+00, -1.9433e-02,\n        -3.1413e-02,  2.3825e+00,  2.5577e+00,  3.8088e+00, -2.1320e+00,\n         5.9358e-01,  4.5791e+00,  2.3627e+00,  1.7950e+00,  2.3262e+00,\n        -8.4886e-01,  7.6425e-01,  3.1207e+00,  3.2152e+00, -4.2184e-02,\n         3.7596e+00, -1.3912e+00, -3.0542e+00, -1.0600e+00, -1.4938e+00,\n         3.4581e+00, -1.3136e+00,  3.5402e+00,  2.5300e+00, -2.1163e+00,\n         2.0785e-01, -1.3936e+00, -2.2232e+00,  2.2485e+00, -1.7186e+00,\n        -1.4675e+00, -5.1313e-01,  5.0823e-01, -2.3642e-01,  6.7068e-01,\n         1.5438e+00, -2.1925e+00,  3.0441e+00, -1.6325e+00, -1.2317e+00,\n         1.7177e+00,  2.1256e+00,  4.3604e+00, -6.4899e-02,  1.2479e+00,\n        -2.6175e+00,  4.6558e+00,  8.8558e-01,  3.7692e+00,  4.2859e+00,\n        -1.6412e-01,  1.7702e+00, -1.8149e+00,  4.1858e+00,  2.1287e+00,\n        -2.3270e+00,  4.4834e-01, -6.7363e-01,  2.0554e+00, -1.1591e+00,\n         1.0113e-01,  4.9411e+00,  1.2790e+00,  4.4911e+00, -2.1018e+00,\n         4.3243e+00,  3.5019e+00,  3.2655e+00, -1.2832e+00,  2.3690e+00,\n         1.5998e+00,  4.4525e+00,  6.8272e-01,  2.5144e+00,  2.5326e+00,\n        -2.5178e+00, -2.5880e+00,  1.1106e-01, -1.3480e+00,  5.0132e+00,\n         2.9592e+00,  2.4288e+00, -1.9502e+00, -2.9371e+00,  6.5316e+00,\n         9.6309e-01,  4.2664e+00, -3.4872e+00, -2.5600e+00, -2.3647e+00,\n         6.9991e-01,  1.8243e+00, -1.1008e+00,  3.7555e+00, -1.8279e+00,\n        -3.4182e-01,  3.3323e-02,  2.5337e-01,  1.2644e+00,  2.0794e+00,\n         4.1897e+00,  4.0726e+00,  1.9493e+00,  1.8511e+00, -4.5626e+00,\n         2.0109e+00, -3.3892e+00, -1.1291e+00, -3.8025e-01, -1.0493e+00,\n         2.9350e+00,  8.7915e-01,  1.6768e+00, -4.9359e-01, -4.6732e-01,\n         2.3426e-02,  1.0919e+01,  2.2375e+00,  3.9439e+00,  4.7312e+00,\n         1.0267e+00,  3.2809e-01,  1.4104e+00, -3.0198e-01,  1.4288e+00,\n         2.1343e+00,  2.3142e+00, -1.3102e+00,  6.0681e+00,  2.2928e+00,\n         8.2694e-01,  5.5357e-01, -1.6876e+00,  3.4213e+00,  4.9468e+00,\n         9.2235e-01, -1.1074e+00,  4.9436e+00, -4.2475e-01,  3.4593e+00,\n         3.3029e+00, -4.1219e-01,  5.3851e+00, -8.2658e-01,  1.4160e+00,\n         1.5981e+00,  1.7853e+00,  2.4361e+00,  6.4131e-01,  1.5073e-01,\n        -2.7599e+00,  3.2856e-02,  2.2710e+00,  1.2456e+00,  3.9124e+00,\n        -1.0529e+00,  1.9322e+00,  7.9575e+00,  2.2481e+00,  8.4153e-01,\n        -1.0565e+00,  3.9485e-01,  6.6475e+00,  3.0284e+00, -1.4537e+00,\n         1.3748e+01,  6.4754e+00,  4.9702e+00,  5.4948e-01, -8.4636e-01,\n        -6.6400e-01,  2.2658e+00,  8.2301e+00,  2.1361e+00, -9.4287e-01,\n        -4.6417e-01,  2.2660e+00,  1.8134e+00,  2.4605e+00, -7.9116e-01,\n        -1.1654e+00, -2.0466e+00,  6.6130e-01,  4.0720e+00,  1.1844e-01,\n        -4.9475e-01,  2.2667e+00, -9.9757e-01,  3.5477e+00,  2.9346e+00,\n        -9.4687e-01, -1.9277e+00,  5.3551e+00,  5.1556e+00,  8.0569e-01,\n        -1.6008e+00,  1.4178e+00, -1.7512e+00, -1.3459e-01, -1.0417e+00,\n         3.5064e+00, -2.9496e-01,  2.9234e+00, -1.0669e+00,  1.1812e-01,\n        -4.4109e-01,  4.6833e+00,  3.9833e+00,  1.3008e+00, -2.1782e+00,\n         3.4082e+00,  5.7158e+00,  2.2401e+00,  5.2952e+00,  7.1684e-01,\n         3.2463e+00, -4.4331e-01, -5.8507e-02,  2.0183e+00, -3.5807e-01,\n         5.9374e-01,  2.5371e+00, -1.9877e+00, -2.8489e+00, -1.4427e-01,\n         3.7440e+00, -1.0832e+00,  5.0348e-02, -3.2139e+00, -9.5426e-01,\n        -9.3383e-02,  2.0879e+00,  1.5619e+00,  5.1383e-01, -5.9829e-01,\n         2.3191e+00,  2.2724e+00, -1.7798e+00,  3.6420e+00,  1.5429e+00,\n         4.1850e+00, -1.9627e+00,  5.2809e+00, -2.1038e+00, -2.0615e+00,\n         4.3211e+00, -4.9216e-01, -2.5677e+00, -9.2093e-01,  2.0140e+00,\n         5.2202e-01, -9.2108e-01,  2.4938e+00,  1.5035e+00, -2.1330e+00,\n         1.6283e+00, -2.3510e-01,  2.3037e+00, -2.2182e+00,  4.3178e+00,\n        -2.0257e+00, -1.1820e+00,  2.5795e+00,  2.4651e+00,  4.0672e+00,\n         1.0538e+00,  6.3162e-01,  3.2792e-01,  2.4784e+00,  8.6255e-01,\n        -2.1936e+00, -5.6981e-01,  2.5956e+00, -2.4254e-02, -2.0705e+00,\n        -2.4443e+00,  2.0327e-01,  2.9463e+00, -7.1701e-01,  5.4296e-01,\n         8.3410e-01,  2.2366e+00, -9.1158e-02,  3.5143e-01, -1.3009e+00,\n         5.7801e-01,  6.8508e+00,  8.7869e+00,  2.4257e+00,  4.8176e-01,\n        -7.7209e-01,  7.3778e+00,  1.5369e+00,  1.4698e+00, -1.4324e+00,\n        -2.2020e+00, -3.3307e+00, -2.5161e+00, -1.4087e-01,  3.5807e-01,\n        -4.1054e+00, -9.7527e-01, -1.2163e+00, -3.9127e+00, -2.6394e+00,\n        -2.3581e+00, -2.0051e+00, -3.2544e+00, -2.2906e+00, -3.1967e+00,\n        -2.6025e+00, -1.8432e+00, -4.1229e+00, -1.6087e+00, -2.5088e+00,\n        -3.8067e+00, -2.9496e+00, -3.3335e+00, -1.0537e+00, -3.3762e+00,\n        -1.4116e+00, -1.5406e+00, -3.3354e+00, -1.1711e+00, -5.0223e-01,\n        -4.9288e+00, -3.0680e+00, -2.3799e+00, -1.6476e+00, -2.2977e+00,\n         3.6939e-01, -6.3917e-01, -4.3213e+00, -1.0377e+00, -4.4969e+00,\n        -2.3729e+00,  4.8973e-01, -2.7332e+00,  1.6064e+00, -2.8189e+00,\n         4.8549e+00,  6.0302e-01, -1.8635e+00, -1.2257e+00, -2.8696e+00,\n        -1.3252e+00, -5.0209e+00,  1.2780e-01, -4.7062e-01, -1.8892e+00,\n        -1.9412e+00,  1.1906e+00,  2.1539e+00,  4.1510e+00,  1.4599e-01,\n        -3.9409e+00, -6.8565e+00, -2.5484e+00, -3.3817e+00, -1.8924e+00,\n        -5.4848e+00, -5.6860e+00, -5.1310e+00, -5.4157e+00, -2.1441e+00,\n        -4.1906e+00, -1.8489e+00, -5.0577e+00, -4.0309e+00,  2.1994e+00])\ntensor([1.5724e-07, 4.1914e-08, 1.2298e-06, 1.1338e-06, 1.9157e-07, 1.3663e-07,\n        4.5767e-07, 3.5362e-07, 1.2608e-07, 9.6500e-08, 7.0591e-07, 1.4593e-07,\n        1.4874e-08, 1.0496e-07, 2.5439e-08, 2.1527e-07, 9.5767e-08, 7.3083e-07,\n        1.0093e-07, 1.6401e-07, 6.5796e-09, 1.4297e-07, 3.1379e-07, 2.0151e-07,\n        1.2775e-06, 2.1143e-06, 6.5174e-08, 2.3350e-07, 1.3283e-06, 4.6594e-09,\n        1.3347e-08, 7.1753e-08, 1.9599e-08, 9.3966e-08, 9.5786e-07, 3.4393e-08,\n        6.2408e-07, 1.1105e-07, 1.0911e-06, 1.1933e-07, 8.2589e-08, 1.9326e-07,\n        5.2675e-07, 6.3526e-08, 3.3918e-08, 1.2244e-06, 8.0673e-08, 8.2059e-08,\n        2.7415e-07, 1.0635e-07, 2.4593e-06, 5.3059e-07, 5.3562e-08, 3.1359e-07,\n        8.2294e-08, 1.5690e-09, 5.2483e-07, 1.4916e-08, 8.1919e-08, 3.5230e-08,\n        1.3550e-07, 2.3024e-07, 7.5571e-08, 1.2606e-07, 6.3927e-09, 6.2708e-07,\n        3.2476e-08, 9.4446e-08, 4.1048e-08, 8.0282e-08, 5.0820e-08, 1.6964e-06,\n        7.3533e-07, 1.5539e-07, 1.1214e-07, 5.0117e-08, 2.0457e-08, 1.0763e-08,\n        2.8450e-07, 1.3856e-07, 1.0540e-06, 5.4515e-07, 1.3888e-07, 2.2617e-07,\n        1.6383e-07, 7.4963e-08, 1.8264e-07, 6.3743e-06, 2.2510e-06, 8.1408e-08,\n        2.0874e-07, 2.8802e-08, 1.3446e-08, 7.4993e-08, 2.8955e-08, 2.5167e-09,\n        2.3583e-08, 5.2431e-09, 2.2461e-09, 2.4602e-08, 1.5407e-07, 6.2579e-07,\n        2.5469e-08, 2.9486e-07, 1.9305e-07, 2.0870e-07, 1.9584e-07, 2.6735e-07,\n        4.7562e-08, 3.8115e-08, 1.6759e-07, 1.1305e-06, 1.3365e-07, 5.6415e-08,\n        1.0089e-08, 5.3852e-08, 1.2559e-08, 5.4725e-08, 1.7512e-06, 1.9126e-07,\n        2.6539e-07, 5.6966e-07, 3.6587e-06, 2.6042e-06, 1.0369e-06, 1.1276e-07,\n        1.8405e-06, 4.2784e-08, 7.3032e-08, 1.4919e-08, 2.7552e-08, 2.6065e-07,\n        1.4690e-07, 3.2520e-08, 7.7277e-08, 9.8850e-08, 2.7371e-08, 5.8465e-08,\n        4.4524e-08, 8.1948e-08, 1.4840e-08, 2.8780e-08, 1.2535e-07, 1.0785e-08,\n        4.9174e-08, 1.7862e-07, 1.9266e-08, 1.2441e-06, 2.6169e-07, 3.8860e-08,\n        2.0385e-07, 6.5172e-06, 7.7937e-08, 1.3305e-07, 4.8587e-07, 4.9639e-07,\n        4.8857e-08, 3.8045e-07, 1.0194e-06, 2.5277e-06, 4.6870e-06, 3.2896e-06,\n        2.4904e-07, 8.3011e-06, 3.6367e-06, 1.3381e-06, 1.0351e-06, 3.9537e-07,\n        4.5500e-07, 2.1528e-05, 1.8062e-05, 7.0539e-06, 1.3053e-04, 1.9617e-06,\n        4.1675e-05, 1.7833e-07, 6.8254e-06, 1.3012e-05, 1.0194e-05, 2.0486e-06,\n        7.3219e-07, 1.0088e-05, 8.7706e-08, 1.9656e-06, 1.5794e-07, 9.6204e-07,\n        8.3442e-07, 1.6008e-06, 5.4976e-07, 7.7305e-07, 5.5122e-07, 4.1799e-07,\n        5.9071e-07, 1.9114e-06, 1.0106e-06, 2.7835e-06, 2.3895e-07, 2.4370e-06,\n        1.6265e-06, 2.6952e-06, 7.5702e-07, 4.4163e-07, 1.2101e-06, 3.4289e-06,\n        2.4301e-07, 1.9866e-06, 8.9071e-07, 4.2206e-07, 1.5402e-06, 2.8394e-07,\n        2.2933e-07, 1.3850e-07, 7.5158e-07, 3.6582e-07, 3.6810e-06, 1.8623e-07,\n        2.3150e-06, 3.7285e-07, 1.7778e-07, 4.4722e-07, 4.6541e-07, 3.4185e-07,\n        3.7568e-06, 2.1775e-06, 2.6175e-05, 3.6763e-06, 8.5465e-06, 2.7231e-07,\n        3.6014e-07, 1.7177e-06, 1.5462e-06, 1.7369e-06, 6.2731e-07, 1.4914e-06,\n        4.8100e-06, 1.0665e-04, 5.5253e-06, 5.0431e-07, 9.2678e-06, 8.0407e-07,\n        4.1426e-06, 3.8074e-05, 7.2085e-07, 2.1777e-06, 1.9745e-06, 9.4178e-07,\n        2.1095e-05, 5.3360e-06, 7.4602e-04, 2.3125e-04, 6.2253e-05, 3.0133e-05,\n        6.5079e-08, 2.7364e-06, 5.0257e-05, 3.4777e-05, 8.4278e-07, 5.2558e-06,\n        2.8459e-06, 2.5410e-07, 3.4849e-07, 3.2367e-05, 2.0044e-07, 1.9420e-06,\n        1.6386e-05, 9.2912e-08, 6.7178e-08, 4.3993e-07, 1.5813e-06, 3.9478e-06,\n        2.5210e-06, 3.3065e-07, 9.6816e-07, 1.5547e-07, 5.1160e-08, 8.7364e-07,\n        2.6078e-08, 4.8688e-08, 1.9055e-08, 7.2444e-07, 2.2817e-07, 6.3132e-07,\n        1.7723e-06, 1.0120e-06, 1.4055e-07, 1.2230e-07, 5.6740e-07, 1.1158e-07,\n        5.6299e-07, 2.8726e-07, 2.4930e-08, 1.9316e-07, 3.0000e-06, 4.5626e-08,\n        4.3778e-07, 3.6189e-07, 3.6515e-06, 4.8775e-08, 3.1033e-09, 6.6115e-08,\n        1.0339e-05, 3.9896e-07, 1.7058e-05, 1.9294e-05, 4.2690e-07, 5.6828e-07,\n        1.1086e-06, 1.4924e-06, 1.5824e-07, 4.7963e-07, 1.3116e-05, 1.4565e-06,\n        4.4958e-07, 5.5460e-07, 2.5324e-06, 5.5604e-07, 2.3512e-08, 1.0573e-07,\n        5.1303e-07, 9.5746e-08, 3.0196e-08, 8.0646e-06, 2.7011e-08, 9.2353e-07,\n        7.2575e-07, 1.7324e-07, 9.1176e-07, 3.5289e-07, 7.5616e-08, 8.9259e-08,\n        1.3501e-07, 1.4931e-07, 7.0413e-08, 3.7947e-07, 6.3190e-09, 8.8447e-09,\n        2.3543e-08, 2.8003e-07, 8.7398e-07, 1.0736e-07, 6.0064e-07, 2.5604e-07,\n        1.6872e-06, 1.7056e-07, 4.2863e-07, 2.5904e-06, 1.1324e-06, 3.6489e-07,\n        5.1092e-07, 1.3792e-06, 1.2603e-06, 2.8640e-07, 5.0062e-07, 1.7864e-07,\n        1.7590e-05, 4.5645e-07, 8.5182e-07, 8.1496e-08, 2.9559e-07, 8.0982e-07,\n        1.2775e-07, 1.0977e-07, 8.8708e-07, 1.6342e-07, 1.9222e-07, 1.9327e-08,\n        3.2832e-08, 5.6070e-07, 1.5340e-08, 9.4613e-09, 7.5863e-08, 1.6226e-07,\n        6.4036e-08, 3.4669e-08, 3.7311e-07, 3.7085e-08, 8.3585e-08, 2.5525e-08,\n        9.7025e-08, 2.5760e-08, 6.6781e-09, 1.4618e-08, 7.9362e-08, 4.9927e-07,\n        1.6329e-07, 3.9363e-06, 3.7799e-07, 5.8611e-09, 1.0737e-07, 2.3835e-07,\n        4.1902e-07, 6.1553e-06, 2.7358e-07, 2.6107e-08, 3.6293e-06, 1.2416e-07,\n        7.5282e-08, 8.5745e-09, 3.6899e-06, 2.8867e-04, 5.0221e-06, 4.8785e-06,\n        1.9636e-05, 6.5524e-08, 3.0684e-08, 3.3664e-07, 3.6045e-06, 1.3669e-07,\n        2.2684e-07, 2.8165e-05, 5.2838e-07, 1.6166e-05, 1.3942e-05, 4.5197e-05,\n        6.7766e-04, 3.2479e-07, 5.7765e-07, 3.4947e-07, 2.3299e-05, 1.0575e-05,\n        2.4421e-04, 3.4381e-06, 1.8339e-07, 1.9005e-06, 3.1261e-06, 1.4423e-06,\n        7.0680e-06, 5.1874e-05, 8.7088e-06, 5.5131e-07, 2.7130e-07, 4.2193e-06,\n        9.0684e-06, 2.8325e-06, 4.4622e-07, 2.2761e-07, 9.1815e-08, 6.8486e-08,\n        9.1490e-08, 1.7212e-06, 3.3501e-06, 2.3190e-07, 4.6375e-07, 3.0011e-05,\n        9.2164e-06, 1.2408e-06, 6.7523e-04, 8.9908e-06, 2.4233e-06, 1.3227e-07,\n        1.6728e-04, 7.5494e-06, 1.8289e-06, 4.7724e-06, 1.4717e-05, 1.0520e-06,\n        2.3145e-04, 9.1268e-06, 9.1620e-06, 1.4454e-05, 1.3795e-07, 2.4756e-05,\n        6.8118e-06, 7.8237e-05, 4.5099e-06, 6.5363e-06, 6.1215e-09, 4.0102e-08,\n        7.6379e-07, 7.6231e-06, 3.9205e-07, 4.3694e-06, 4.8631e-06, 3.3016e-07,\n        4.8336e-07, 5.8474e-06, 3.2609e-07, 4.0154e-05, 5.6701e-07, 1.2889e-06,\n        5.9533e-07, 5.8580e-05, 8.4330e-06, 6.9424e-07, 3.4869e-07, 1.2977e-06,\n        1.2682e-05, 9.9104e-06, 3.6839e-05, 1.1310e-06, 3.1489e-05, 2.1583e-04,\n        1.2780e-06, 3.6534e-07, 3.8125e-04, 8.4524e-07, 1.9250e-05, 6.0883e-06,\n        3.6622e-07, 3.4611e-06, 9.1162e-08, 6.3720e-05, 2.8062e-06, 9.8859e-06,\n        6.6082e-06, 2.4686e-07, 6.6988e-08, 1.3995e-06, 6.0460e-07, 3.9728e-08,\n        2.1439e-08, 5.9306e-08, 8.3780e-06, 1.2790e-04, 1.9449e-05, 2.9359e-05,\n        1.2372e-05, 4.1711e-07, 1.4104e-05, 6.3588e-08, 5.5659e-06, 1.7804e-07,\n        6.9215e-07, 5.6970e-04, 1.4374e-06, 7.6215e-08, 5.9752e-06, 1.1788e-06,\n        2.9976e-06, 3.1178e-06, 9.4192e-07, 7.1698e-06, 5.7944e-07, 2.6338e-07,\n        6.6691e-07, 1.6369e-06, 2.1922e-07, 2.6417e-02, 3.7741e-07, 1.5790e-06,\n        4.2321e-07, 5.3951e-05, 1.1151e-04, 1.2503e-07, 3.3582e-06, 4.7414e-06,\n        5.5160e-05, 2.2940e-08, 1.1570e-06, 3.1282e-04, 1.0128e-08, 3.7747e-06,\n        3.5025e-06, 1.6402e-06, 1.5083e-07, 3.3174e-07, 7.4774e-06, 5.7020e-06,\n        2.8512e-05, 2.8028e-04, 7.0054e-06, 1.5555e-06, 5.8258e-07, 6.9111e-06,\n        8.2864e-08, 2.2289e-07, 5.0021e-05, 6.1082e-07, 6.4968e-07, 6.8220e-07,\n        2.0793e-04, 1.2744e-06, 5.2340e-07, 1.4234e-05, 3.8237e-07, 1.2879e-06,\n        1.8562e-05, 7.1518e-05, 3.2773e-06, 1.1092e-06, 1.2745e-07, 2.8667e-07,\n        4.8146e-08, 1.3432e-06, 6.8687e-05, 1.6395e-05, 1.2461e-06, 9.4015e-06,\n        1.1689e-06, 5.9836e-06, 2.8224e-06, 4.1445e-05, 2.7252e-06, 3.0477e-05,\n        4.6362e-06, 2.3333e-08, 9.0570e-06, 1.0335e-06, 1.1722e-06, 6.6578e-07,\n        2.7261e-05, 2.1060e-06, 1.8011e-06, 2.8705e-06, 1.5762e-07, 8.4374e-06,\n        3.9769e-05, 2.9366e-08, 2.0137e-05, 2.1763e-07, 2.8135e-04, 9.8705e-05,\n        1.6317e-05, 6.1276e-07, 3.1004e-04, 3.0425e-05, 1.2819e-06, 4.4984e-06,\n        7.6242e-06, 9.6520e-07, 1.7711e-05, 1.3891e-05, 7.0834e-06, 1.3129e-05,\n        2.5563e-06, 2.2644e-06, 8.5219e-06, 7.5786e-08, 2.7125e-07, 9.3373e-07,\n        9.2261e-07, 1.0312e-05, 1.2287e-05, 4.2935e-05, 1.1291e-07, 1.7236e-06,\n        9.2755e-05, 1.0110e-05, 5.7310e-06, 9.7477e-06, 4.0739e-07, 2.0444e-06,\n        2.1575e-05, 2.3714e-05, 9.1273e-07, 4.0872e-05, 2.3685e-07, 4.4898e-08,\n        3.2985e-07, 2.1375e-07, 3.0234e-05, 2.5596e-07, 3.2820e-05, 1.1952e-05,\n        1.1470e-07, 1.1720e-06, 2.3629e-07, 1.0307e-07, 9.0192e-06, 1.7072e-07,\n        2.1946e-07, 5.6992e-07, 1.5826e-06, 7.5160e-07, 1.8618e-06, 4.4577e-06,\n        1.0628e-07, 1.9984e-05, 1.8607e-07, 2.7782e-07, 5.3047e-06, 7.9759e-06,\n        7.4531e-05, 8.9223e-07, 3.3159e-06, 6.9484e-08, 1.0015e-04, 2.3081e-06,\n        4.1267e-05, 6.9186e-05, 8.0795e-07, 5.5907e-06, 1.5505e-07, 6.2593e-05,\n        8.0008e-06, 9.2908e-08, 1.4906e-06, 4.8541e-07, 7.4352e-06, 2.9873e-07,\n        1.0534e-06, 1.3322e-04, 3.4208e-06, 8.4945e-05, 1.1638e-07, 7.1895e-05,\n        3.1588e-05, 2.4937e-05, 2.6386e-07, 1.0174e-05, 4.7145e-06, 8.1728e-05,\n        1.8844e-06, 1.1766e-05, 1.1983e-05, 7.6773e-08, 7.1566e-08, 1.0639e-06,\n        2.4731e-07, 1.4318e-04, 1.8359e-05, 1.0801e-05, 1.3542e-07, 5.0476e-08,\n        6.5358e-04, 2.4942e-06, 6.7845e-05, 2.9119e-08, 7.3600e-08, 8.9471e-08,\n        1.9170e-06, 5.9010e-06, 3.1667e-07, 4.0707e-05, 1.5305e-07, 6.7641e-07,\n        9.8431e-07, 1.2266e-06, 3.3710e-06, 7.6160e-06, 6.2838e-05, 5.5897e-05,\n        6.6867e-06, 6.0617e-06, 9.9345e-09, 7.1117e-06, 3.2119e-08, 3.0782e-07,\n        6.5091e-07, 3.3338e-07, 1.7920e-05, 2.2934e-06, 5.0919e-06, 5.8116e-07,\n        5.9663e-07, 9.7462e-07, 5.2583e-02, 8.9204e-06, 4.9145e-05, 1.0799e-04,\n        2.6581e-06, 1.3217e-06, 3.9010e-06, 7.0390e-07, 3.9735e-06, 8.0458e-06,\n        9.6322e-06, 2.5684e-07, 4.1114e-04, 9.4275e-06, 2.1767e-06, 1.6560e-06,\n        1.7610e-07, 2.9142e-05, 1.3397e-04, 2.3946e-06, 3.1459e-07, 1.3354e-04,\n        6.2258e-07, 3.0270e-05, 2.5886e-05, 6.3045e-07, 2.0768e-04, 4.1656e-07,\n        3.9231e-06, 4.7065e-06, 5.6754e-06, 1.0881e-05, 1.8079e-06, 1.1069e-06,\n        6.0261e-08, 9.8385e-07, 9.2249e-06, 3.3084e-06, 4.7621e-05, 3.3220e-07,\n        6.5737e-06, 2.7200e-03, 9.0157e-06, 2.2087e-06, 3.3101e-07, 1.4130e-06,\n        7.3391e-04, 1.9673e-05, 2.2250e-07, 8.8962e-01, 6.1784e-04, 1.3715e-04,\n        1.6493e-06, 4.0840e-07, 4.9011e-07, 9.1765e-06, 3.5723e-03, 8.0602e-06,\n        3.7083e-07, 5.9852e-07, 9.1787e-06, 5.8371e-06, 1.1149e-05, 4.3158e-07,\n        2.9686e-07, 1.2298e-07, 1.8444e-06, 5.5859e-05, 1.0718e-06, 5.8049e-07,\n        9.1853e-06, 3.5109e-07, 3.3068e-05, 1.7912e-05, 3.6935e-07, 1.3851e-07,\n        2.0153e-04, 1.6509e-04, 2.1309e-06, 1.9206e-07, 3.9302e-06, 1.6525e-07,\n        8.3217e-07, 3.3593e-07, 3.1730e-05, 7.0886e-07, 1.7712e-05, 3.2756e-07,\n        1.0714e-06, 6.1249e-07, 1.0294e-04, 5.1120e-05, 3.4963e-06, 1.0781e-07,\n        2.8761e-05, 2.8906e-04, 8.9436e-06, 1.8982e-04, 1.9498e-06, 2.4463e-05,\n        6.1113e-07, 8.9795e-07, 7.1649e-06, 6.6551e-07, 1.7239e-06, 1.2037e-05,\n        1.3044e-07, 5.5133e-08, 8.2415e-07, 4.0242e-05, 3.2228e-07, 1.0012e-06,\n        3.8271e-08, 3.6663e-07, 8.6717e-07, 7.6809e-06, 4.5394e-06, 1.5915e-06,\n        5.2339e-07, 9.6786e-06, 9.2375e-06, 1.6059e-07, 3.6336e-05, 4.4539e-06,\n        6.2542e-05, 1.3374e-07, 1.8713e-04, 1.1615e-07, 1.2117e-07, 7.1662e-05,\n        5.8199e-07, 7.3037e-08, 3.7906e-07, 7.1339e-06, 1.6046e-06, 3.7900e-07,\n        1.1527e-05, 4.2818e-06, 1.1280e-07, 4.8511e-06, 7.5259e-07, 9.5311e-06,\n        1.0359e-07, 7.1429e-05, 1.2558e-07, 2.9196e-07, 1.2559e-05, 1.1200e-05,\n        5.5594e-05, 2.7310e-06, 1.7905e-06, 1.3215e-06, 1.1351e-05, 2.2556e-06,\n        1.0617e-07, 5.3851e-07, 1.2763e-05, 9.2924e-07, 1.2007e-07, 8.2626e-08,\n        1.1667e-06, 1.8123e-05, 4.6480e-07, 1.6386e-06, 2.1923e-06, 8.9124e-06,\n        8.6910e-07, 1.3530e-06, 2.5923e-07, 1.6970e-06, 8.9935e-04, 6.2341e-03,\n        1.0768e-05, 1.5413e-06, 4.3989e-07, 1.5233e-03, 4.4270e-06, 4.1398e-06,\n        2.2729e-07, 1.0528e-07, 3.4052e-08, 7.6898e-08, 8.2696e-07, 1.3620e-06,\n        1.5693e-08, 3.5901e-07, 2.8211e-07, 1.9028e-08, 6.7981e-08, 9.0062e-08,\n        1.2820e-07, 3.6753e-08, 9.6350e-08, 3.8936e-08, 7.0538e-08, 1.5072e-07,\n        1.5421e-08, 1.9055e-07, 7.7464e-08, 2.1156e-08, 4.9850e-08, 3.3958e-08,\n        3.3192e-07, 3.2538e-08, 2.3207e-07, 2.0397e-07, 3.3894e-08, 2.9515e-07,\n        5.7616e-07, 6.8880e-09, 4.4282e-08, 8.8118e-08, 1.8328e-07, 9.5668e-08,\n        1.3775e-06, 5.0243e-07, 1.2645e-08, 3.3727e-07, 1.0609e-08, 8.8738e-08,\n        1.5536e-06, 6.1892e-08, 4.7460e-06, 5.6809e-08, 1.2221e-04, 1.7400e-06,\n        1.4769e-07, 2.7946e-07, 5.4004e-08, 2.5301e-07, 6.2822e-09, 1.0818e-06,\n        5.9466e-07, 1.4394e-07, 1.3666e-07, 3.1313e-06, 8.2055e-06, 6.0454e-05,\n        1.1017e-06, 1.8500e-08, 1.0022e-09, 7.4458e-08, 3.2360e-08, 1.4348e-07,\n        3.9504e-09, 3.2305e-09, 5.6273e-09, 4.2331e-09, 1.1155e-07, 1.4412e-08,\n        1.4987e-07, 6.0551e-09, 1.6907e-08, 8.5874e-06])\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'wget' is not recognized as an internal or external command,\noperable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt -OutFile imagenet_classes.txt"
   ]
  }
 ]
}